---
title: Workflow
---

```{r setup, include=FALSE}
#| file: functions.R
```

## Operational Product Development Timeline

Over the course of the year, the survey team is developing a variety of different data products. Planning and preparation for surveys happens in the late winter and spring, surveys occur in the summer, data validation takes place over the course of the survey and after the survey, and data products are produced through fall and late winter.  

```{r}
#| tbl-cap: Operational product development timeline. 
#| label: prod-timeline

dat <- data.frame(Month = unique(months(as.Date(x = 1:365, format = "%m"))), 
           Surveys = 
             c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0), 
           Planning = 
             c(1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1), 
           Development = 
             c(1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1), 
           deployment_deliverables = # Deployment (survey deliverables)
             c(0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1), 
           deployment_operations = # Deployment (survey operations)
             c(0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0), 
           triage = 1, # Triage (fixing bugs and errors)
           feedback = 1 # User feedback and brainstorming
           ) %>% 
  t() %>% 
  data.frame()

names(dat) <- dat[1,]
dat <- dat[-1,]
cols  <- viridis_pal(option = "G", begin = .2, end = .8)(nrow(dat))
# dat$cols <- cols
dat[dat == 0] <- ""
# for (i in 1:nrow(dat)) {
#   dat[i,][dat == TRUE] <- rownames(dat)[i]
# }
dat <- cbind.data.frame(" " = c("Surveys", "Planning", "Development", "Deployment (survey deliverables)", "Deployment (survey operations)", "Triage (fixing bugs and errors)", "User feedback and brainstorming"), 
                        dat)
t0 <- dat %>% 
  flextable::flextable() %>% # col_keys = names(dat)[-ncol(dat)]
  flextable::vline(j = 1)

for (i in 1:nrow(dat)){
  for (j in 2:(ncol(dat))) {
    if (dat[i,j] == 1) {
t0 <- t0 %>% 
  flextable::bg(i = i, j = j, bg = cols[i], part = "body")  %>% 
  flextable::color(i = i, j = j, color = cols[i], part = "body") 
    }
  }
}

t0
```

## Data workflow from boat to production

Organisms first need to be collected aboard the vessel before data can be entered into tablets. 



The objective of this process is to take raw data, QA/QC and clean these data, curate standard data products for these survey. Please note, through this process we are not providing "data" (what we consider lower level data material; see the data levels section below) but "data products", which is intended to facilitate the most fool-proof standard interpretation of the data. These data products only use data from standard and validated hauls, and has undergone careful review. 

**Once survey data collected on the vessel has been checked and validated**, the [`gap_products/code/run.R`](https://github.com/afsc-gap-products/gap_products/blob/main/code/run.R) script is used to orchestrate a sequence of programs that calculate the standard data products resulting from the NOAA AFSC GAP bottom trawl surveys. Standard data products are the CPUE, BIOMASS, SIZECOMP, and AGECOMP tables in the `GAP_PRODUCTS` Oracle schema. The tables are slated to be updated twice a year: once after the survey season following finalization of that summer's bottom trawl survey data to incorporate the new catch, size, and effort data and once prior to an upcoming survey to incorporate new age data that were processed after the prior summer's survey season ended. This second pre-survey production run will also incorporate changes in the data due to the specimen voucher process as well as other post-hoc changes in the survey data. 

> The data from these surveys constitute a **living data set** so we can continue to **provide the best available data to all partners, stakeholders, and fellow scientists**. 



During each data product run cycle:

1.  Versions of the tables in GAP_PRODUCTS are locally imported within the gap_products repository to compare with the updated production tables. Any changes to a production table will be compared and checked to make sure those changes are intentional and documented.

2.  Use the `gapindex` R package to calculate the four major standard data products: CPUE, BIOMASS, SIZECOMP, AGECOMP. These tables are compared and checked to their respective locally saved copies and any changes to the tables are vetted and documented. These tables are then uploaded to the GAP_PRODUCTS Oracle schema.

3.  Calculate the various materialized views for AKFIN and FOSS purposes. Since these are derivative of the tables in GAP_PRODUCTS as well as other base tables in RACEBASE and RACE_DATA, it is not necessary to check these views in addition to the data checks done in the previous steps.


